{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73650c01-47e3-49b2-9207-04f0ce6eca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackyhe/anaconda3/envs/rag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import base64\n",
    "import pypdfium2 as pdfium\n",
    "from openai import OpenAI\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "api_url = 'https://www.chatmol.org/ollama/api/generate'\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        \n",
    "# LLM_CLIENT\n",
    "def get_llm_client(provider):\n",
    "    # OpenAI client\n",
    "    if (provider == \"OpenAI\"):\n",
    "        openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "    # DeepSeek client\n",
    "    elif (provider == \"DeepSeek\"):\n",
    "        ds_api_key = os.environ[\"DS_API_KEY\"]\n",
    "        # model: deepseek-chat, 128k context window size, 8k max output tokens\n",
    "        client = OpenAI(api_key=ds_api_key, base_url=\"https://api.deepseek.com\")\n",
    "    # Ollama client\n",
    "    elif (provider == \"Ollama\"):\n",
    "        # Using OpenAI interface example\n",
    "        client = OpenAI(\n",
    "            # base_url = 'https://www.chatmol.org/ollama/v1/',\n",
    "            base_url = 'http://100.89.180.132:11434/v1/',\n",
    "            api_key='ollama',  # required but ignored\n",
    "        )\n",
    "    else:\n",
    "        print(\"Unknown LLM provider\")\n",
    "        client = None\n",
    "    return client\n",
    "\n",
    "#OpenAI client\n",
    "# client = get_llm_client(provider=\"OpenAI\")\n",
    "# client = get_llm_client(provider=\"DeepSeek\")\n",
    "# client = get_llm_client(provider=\"Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9836eca2-8e22-4262-8354-e26f67988c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_headings(doc_md, llm_client, llm_model):\n",
    "    # Create the data payload\n",
    "    prompt = \"\"\"In the following markdown text, all the headers are on the same level. The top level headers are sections. Some sections may have\n",
    "    sub-sections or even sub-sub-sections. Please set the header levels correctly according to the content structures. For simplicity in your \n",
    "    output, you can only response with all headers. Please consider the following rules:\n",
    "    \n",
    "    1. Let's start from level 2, like: ## <seciton_header>\n",
    "    2. If the section header has a number, please also keep the number.\n",
    "    3. Please don't add anything (such as level) that is not in the original headers. \n",
    "    \"\"\"\n",
    "    max_tokens = 2048\n",
    "    responses = llm_client.chat.completions.create(\n",
    "        model = llm_model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\"content\": prompt},\n",
    "            {\"role\": \"user\",\"content\": f\"Here is the current markdown text:\\n\\n{doc_md}\"},\n",
    "            {\"role\": \"user\", \"content\": \"Please export correct markdow headings, each per line.\"},\n",
    "        ],\n",
    "        temperature = 0.0,\n",
    "        max_tokens = max_tokens,\n",
    "    )\n",
    "\n",
    "    new_headings = responses.choices[0].message.content\n",
    "    return new_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd6bc7c-95ae-4160-8156-d230acc59a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_headings(original_markdown, correct_headings):\n",
    "    # Split headers into a list\n",
    "    correct_heading_list = correct_headings.strip().split('\\n')\n",
    "    \n",
    "    # Generate a mapping of old to new headers\n",
    "    header_mapping = {}\n",
    "    \n",
    "    for new_header in correct_heading_list:\n",
    "        # Extract the header text without the markdown levels\n",
    "        header_text = new_header.lstrip('# ').strip()\n",
    "        # Create a regex to find headers with varying levels\n",
    "        regex = re.compile(r'^(#{1,6}\\s*)' + re.escape(header_text) + r'$', re.MULTILINE)\n",
    "        # Replace all occurrences with the correct level\n",
    "        header_mapping[regex] = new_header\n",
    "    \n",
    "    # Replace headers in the original markdown\n",
    "    updated_markdown = original_markdown\n",
    "    for pattern, replacement in header_mapping.items():\n",
    "        updated_markdown = pattern.sub(replacement, updated_markdown)\n",
    "    \n",
    "    return updated_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7443702-ed1d-4853-9149-dcc153ec82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docling_pdf_parser(pdf_source):\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(pdf_source)\n",
    "    print(\"Done with docling convert\")\n",
    "    raw_md = result.document.export_to_markdown()\n",
    "    return raw_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec74def-1d49-408d-b32b-10ff5a77c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Parser: convert PDF into markdown format using visual LLMs\n",
    "def llm_pdf_parser(pdf_file_path, client, model):\n",
    "    prompt = \"\"\"\n",
    "    You are an expert to convert a PDF file of a scientific paper into markdown text. This markdown text from the PDF should match the structure of the the \n",
    "    content in PDF. Only export pure markdown and nothing else. Do not explain the output. All headerings will start with ##, ###, ####, and so on. \n",
    "\n",
    "    A scientific paper usually includes a title of the paper, a list of authors and their affiliations. Please extract all of them\n",
    "\n",
    "    Don't add any extra headings if not in the original PDF. For example, don't add a heading of continuation. \n",
    "\n",
    "    Don't add extra marks in your output, such as '```markdown'!\n",
    "\n",
    "    Don't include page numbers in the markdown, don't use page numbers as markdown headings.\n",
    "\n",
    "    If you see a table in PDF, convert it into a markdown table. If there is a table title, put the table content immediately after the table \n",
    "    title. If there are notes of the table, also put the notes immediately after the table without blank line. \n",
    "    \n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(pdf_file_path)\n",
    "    pages = pdfium.PdfDocument(pdf_file_path)\n",
    "    n_pages = len(pages)\n",
    "    images_b64 = []\n",
    "    n_dpi = 108\n",
    "    max_tokens = 2048\n",
    "\n",
    "    # Have some overlap \n",
    "    windows = 5\n",
    "    batch_size = 1\n",
    "    n_batch = int(n_pages/batch_size)\n",
    "    if (n_pages > n_batch*batch_size):\n",
    "        n_batch += 1\n",
    "\n",
    "    pre_batch_text = ''\n",
    "    page_counter = 0\n",
    "    image_contents = []\n",
    "    token_usage = 0\n",
    "\n",
    "    md_text = \"\"\n",
    "\n",
    "    for k in range(n_batch):\n",
    "        nstart = k*batch_size\n",
    "        nend = nstart + batch_size\n",
    "        if (nend > n_pages):\n",
    "            nend = n_pages\n",
    "        current_batch_text = \"\"\n",
    "        image_contents = []\n",
    "        for i in range(nstart, nend):\n",
    "            page = pages[i]\n",
    "            page_counter += 1\n",
    "            p_number = i+1\n",
    "            image = page.render(scale = n_dpi/72).to_pil()\n",
    "            image.save('tmp_image.jpeg',\"JPEG\")\n",
    "            b64_image = encode_image('tmp_image.jpeg')\n",
    "            image_item = [{\"type\": \"text\",\"text\": f\"This is page {p_number}\"},\n",
    "                          {\"type\": \"image_url\", \"image_url\": {\n",
    "                              \"url\": f\"data:image/png;base64,{b64_image}\"}\n",
    "                          }]\n",
    "            image_contents += image_item\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": image_contents}]\n",
    "        if (p_number == 1):\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Please extract all text in each page, including the title of the paper, the author list and their contact information\"})\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Please extract all text in each page\"})\n",
    "\n",
    "        responses = client.chat.completions.create(\n",
    "            model = model,\n",
    "            messages = messages,\n",
    "            temperature = 0.0,\n",
    "            max_tokens = max_tokens,\n",
    "        )\n",
    "        current_batch_text = responses.choices[0].message.content\n",
    "        print(\"Finish reason\", responses.choices[0].finish_reason)\n",
    "        token_usage += responses.usage.total_tokens\n",
    "\n",
    "        # Check if the generation is done for the current batch\n",
    "        while (response.choices[0].finish_reason != \"stop\"):\n",
    "            responses = client.chat.completions.create(\n",
    "                model = model,\n",
    "                messages = [\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": image_contents},\n",
    "                {\"role\": \"user\", \"content\": \"This is the markdown generated from the PDF so far:\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{pre_batch_text + current_batch_text}\"},\n",
    "                {\"role\": \"user\", \"content\": \"Please complete the remaining markdown content.\"},\n",
    "                ],\n",
    "                temperature = 0.0, \n",
    "                max_tokens = max_tokens,\n",
    "            )\n",
    "            md_text2 = responses.choices[0].message.content\n",
    "            current_batch_text += md_text2\n",
    "            token_usage += responses.usage.total_tokens\n",
    "        md_text += current_batch_text + \"\\n\"\n",
    "        pre_batch_text = current_batch_text\n",
    "    return md_text, token_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a29139-6b83-45e0-8783-e022a18d6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_markdown(pdf_source, method='docling', reflection_provider=\"Ollama\", reflection_model=\"llama33-16k:latest\"):\n",
    "    if (method == 'docling'):\n",
    "        raw_md = docling_pdf_parser(pdf_source)\n",
    "    else:\n",
    "        client = get_llm_client(\"OpenAI\")\n",
    "        model = 'gpt-4o'\n",
    "        raw_md, token_usage = llm_pdf_parser(pdf_source, client, model)\n",
    "    # Self-reflection for markdown heading corrections\n",
    "    reflection_client = get_llm_client(reflection_provider)\n",
    "    new_headings = get_correct_headings(raw_md,reflection_client,reflection_model)\n",
    "    print(new_headings)\n",
    "    doc_md = replace_headings(raw_md, new_headings) \n",
    "    return doc_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d102b18-cf5a-469c-88a8-0ac9eb606aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_file = \"2024.langmol-1.7.pdf\"\n",
    "# time1 = time.time()\n",
    "# doc_md = pdf_to_markdown(pdf_file, 'docling', 'OpenAI', 'gpt-4o')\n",
    "# print(\"Time = \", time.time()-time1)\n",
    "# print(doc_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d925f0-64ba-4bb6-8d44-cb6ddc4db727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #source = \"https://arxiv.org/pdf/2408.09869\"\n",
    "# pdf_source = \"https://aclanthology.org/2024.langmol-1.7.pdf\"\n",
    "# time1 = time.time()\n",
    "# # DeepSeek V3 context window size upto 128k\n",
    "# # Output size: \n",
    "# doc_md = pdf_to_markdown(pdf_source, 'docling', 'DeepSeek', 'deepseek-chat')\n",
    "# print(\"Time = \", time.time()-time1)\n",
    "# print(doc_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f12ef74-95da-4a4a-bd31-5bdb7673fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #source = \"https://arxiv.org/pdf/2408.09869\"\n",
    "# pdf_source = \"https://aclanthology.org/2024.langmol-1.7.pdf\"\n",
    "# time1 = time.time()\n",
    "# # llama3.3:70B\n",
    "# doc_md = pdf_to_markdown(pdf_source, 'docling', 'Ollama', 'llama33-16k:latest')\n",
    "# print(\"Time = \", time.time()-time1)\n",
    "# print(doc_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1df573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Bio_34_DeepSeek.md, already exists.\n",
      "Skipping Bio_34_Ollama.md, already exists.\n",
      "Skipping CS_8_DeepSeek.md, already exists.\n",
      "Skipping CS_8_Ollama.md, already exists.\n",
      "Done with docling convert\n",
      "## Regional and institutional trends in assessment for academic promotion  \n",
      "## Article  \n",
      "## Study design  \n",
      "## General outlook of promotion criteria  \n",
      "## Regional and institutional differences  \n",
      "## Main trends in assessment  \n",
      "## Factors influencing policy criteria  \n",
      "## Discussion  \n",
      "## Conclusions  \n",
      "## Online content  \n",
      "## Methods  \n",
      "## Data acquisition  \n",
      "## Data preparation  \n",
      "## Data analysis and visualization  \n",
      "## Reporting summary  \n",
      "## Data availability  \n",
      "## Code availability  \n",
      "## Extended Data Fig. 1 | Approach to assessment of research outputs by policy scope  \n",
      "## Extended Data Fig. 2 | Scree plot for the factor analysis  \n",
      "## Extended Data Fig. 3 | Loading plots for each pair of factors  \n",
      "## Extended Data Fig. 4 | Single variable comparison between Global North and South  \n",
      "## Extended Data Table 1 | Description of the sample and its distribution by Region, Economic Status, Disciplines, and Tracks  \n",
      "## Extended Data Table 2 | Attributes of the documents describing promotion policies  \n",
      "## Extended Data Table 3 | Evaluation criteria classified by the sub-categories defined by the authors  \n",
      "## Extended Data Table 4 | Heatmap of the correlation among the criteria in each policy  \n",
      "## Extended Data Table 5 | Differences in the distribution of each factor, between categories of policies  \n",
      "## Extended Data Table 6 | Full Factor loadings\n",
      "Processed Sociology_26.pdf with deepseek-chat on DeepSeek in 61.45 seconds.\n",
      "Done with docling convert\n",
      "## In the following markdown text, all the headers are on the same level\n",
      "## The top level headers are sections\n",
      "## Some sections may have sub-sections or even sub-sub-sections\n",
      "## Please set the header levels correctly according to the content structures\n",
      "## For simplicity in your output, you can only response with all headers\n",
      "## Please consider the following rules:\n",
      "## 1. Let's start from level 2, like: ## <seciton_header>\n",
      "## 2. If the section header has a number, please also keep the number.\n",
      "## 3. Please don't add anything (such as level) that is not in the original headers.\n",
      "Processed Sociology_26.pdf with llama33-16k:latest on Ollama in 83.34 seconds.\n",
      "Done with docling convert\n",
      "## EMPIRICAL STUDY  \n",
      "## A Generative Approach to the Instructed Second Language Acquisition of Spanish se  \n",
      "## Introduction  \n",
      "## Background Literature  \n",
      "## Generative Theory  \n",
      "## Verb Argument Structure  \n",
      "## The Current Study  \n",
      "## Spanish se  \n",
      "## Generative Linguistics and Instructed SLA  \n",
      "## Research Questions  \n",
      "## Method  \n",
      "## Participants  \n",
      "## Materials  \n",
      "## Acceptability Judgment Task (AJT)  \n",
      "## Guided Production Task  \n",
      "## Instructional Materials and Procedure  \n",
      "## Results  \n",
      "## Statistical Approach  \n",
      "## Acceptability Judgment Task  \n",
      "## AJT: Descriptive Statistics  \n",
      "## AJT: Statistical Analysis  \n",
      "## Guided Production Task  \n",
      "## Guided Production Data: Descriptive Statistics  \n",
      "## Guided Production Data: Statistical Analysis  \n",
      "## Discussion  \n",
      "## Acceptability Judgment Data  \n",
      "## Guided Production Task  \n",
      "## General Discussion  \n",
      "## Limitations and Future Directions  \n",
      "## Conclusion  \n",
      "## Notes  \n",
      "## Open Research Badges  \n",
      "## References  \n",
      "## Supporting Information  \n",
      "## Appendix: Accessible Summary\n",
      "Processed Linguistics_42.pdf with deepseek-chat on DeepSeek in 51.71 seconds.\n",
      "Done with docling convert\n",
      "## In the following markdown text, all the headers are on the same level\n",
      "## The top level headers are sections\n",
      "## Some sections may have sub-sections or even sub-sub-sections\n",
      "## Please set the header levels correctly according to the content structures\n",
      "## For simplicity in your output, you can only response with all headers\n",
      "## Please consider the following rules:\n",
      "## 1. Let's start from level 2, like: ## <seciton_header>\n",
      "## 2. If the section header has a number, please also keep the number.\n",
      "## 3. Please don't add anything (such as level) that is not in the original headers.\n",
      "Processed Linguistics_42.pdf with llama33-16k:latest on Ollama in 53.67 seconds.\n",
      "Done with docling convert\n",
      "## OPEN ACCESS  \n",
      "## CITATION  \n",
      "## COPYRIGHT  \n",
      "## Personality and help-seeking for psychological distress: a systematic review and meta-analysis  \n",
      "## 1 Introduction  \n",
      "## 2 Methods  \n",
      "## 2.1 Eligibility criteria  \n",
      "## 2.1.1 Types of records  \n",
      "## 2.1.2 Participants  \n",
      "## 2.1.3 Personality: de /uniFB01 nition and measures  \n",
      "## 2.1.4 Help-seeking for psychological distress: de /uniFB01 nitions and measures  \n",
      "## 2.2 Information sources  \n",
      "## 2.3 Search strategy  \n",
      "## 2.3.1 Record retrieval and deduplication  \n",
      "## 2.3.2 Screening process  \n",
      "## 2.4 Data extraction  \n",
      "## 2.5 Risk of bias assessment  \n",
      "## 2.6 Data synthesis methods  \n",
      "## 3 Results  \n",
      "## 3.1 Overview and study characteristics  \n",
      "## 3.1.1 Screening overview  \n",
      "## 3.1.2 Final set of records  \n",
      "## 3.1.3 Study characteristics  \n",
      "## 3.1.4 Assessment of personality  \n",
      "## 3.1.5 Assessment of help-seeking  \n",
      "## 3.2 Assessment of risk of bias  \n",
      "## 3.3 Synthesized /uniFB01 ndings by personality construct  \n",
      "## 3.3.1 DSM/ICD personality disorders  \n",
      "## 3.3.1.1 Cluster A personality disorders  \n",
      "## 3.3.1.2 Cluster B personality disorders  \n",
      "## 3.3.1.3 Cluster C personality disorders  \n",
      "## 3.3.1.4 Personality disorders from former classi /uniFB01 cations (DSM-III/ICD-10)  \n",
      "## 3.3.2 Five factor personality dimensions  \n",
      "## 3.3.2.1 Neuroticism  \n",
      "## 3.3.2.2 Extraversion  \n",
      "## 3.3.2.3 Openness to experience  \n",
      "## 3.3.2.4 Agreeableness  \n",
      "## 3.3.2.5 Conscientiousness  \n",
      "## 3.3.2.6 Con /uniFB01 rmation of main trends by meta-analysis for help-seeking attitudes  \n",
      "## 3.3.3 Other personality traits not belonging to DSM/ICD personality disorders or to the Five Factor Model  \n",
      "## 4 Discussion  \n",
      "## 4.1 Summary of main /uniFB01 ndings  \n",
      "## 4.2 Integration of /uniFB01 ndings regarding help-seeking attitudes and behavior  \n",
      "## 4.3 Strengths and limitations  \n",
      "## 5 Conclusions  \n",
      "## Author contributions  \n",
      "## Funding  \n",
      "## Acknowledgments  \n",
      "## Con /uniFB02 ict of interest  \n",
      "## Publisher ' s note  \n",
      "## Supplementary material  \n",
      "## References\n",
      "Processed Psychology_20.pdf with deepseek-chat on DeepSeek in 49.28 seconds.\n",
      "Done with docling convert\n",
      "## In the following markdown text, all the headers are on the same level\n",
      "## The top level headers are sections\n",
      "## Some sections may have sub-sections or even sub-sub-sections\n",
      "## Please set the header levels correctly according to the content structures\n",
      "## For simplicity in your output, you can only response with all headers\n",
      "## Please consider the following rules:\n",
      "## 1. Let's start from level 2, like: ## <seciton_header>\n",
      "## 2. If the section header has a number, please also keep the number.\n",
      "## 3. Please don't add anything (such as level) that is not in the original headers.\n",
      "Processed Psychology_20.pdf with llama33-16k:latest on Ollama in 43.89 seconds.\n",
      "Done with docling convert\n",
      "## Annual Review of Political Science  \n",
      "## Political Misinformation  \n",
      "## Jennifer Jerit and Yangzi Zhao  \n",
      "## Keywords  \n",
      "## Abstract  \n",
      "## INTRODUCTION  \n",
      "## WHATIS POLITICAL MISINFORMATION?  \n",
      "## WHATCAUSES POLITICAL MISINFORMATION?  \n",
      "## OVERCOMING MISINFORMATION THROUGH CORRECTION  \n",
      "### The Role of Issue Type  \n",
      "### The Source of the Corrective Message  \n",
      "### Remaining Gaps in the Literature  \n",
      "### Lessons from Psychology  \n",
      "## MEASUREMENT: CONFIDENTLY WRONG OR EXPRESSIVE RESPONDING?  \n",
      "## CONCLUSION  \n",
      "## DISCLOSURE STATEMENT  \n",
      "## ACKNOWLEDGMENTS  \n",
      "## LITERATURE CITED\n",
      "Processed Political_18.pdf with deepseek-chat on DeepSeek in 16.96 seconds.\n",
      "Done with docling convert\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': 'POST predict: Post \"http://127.0.0.1:42311/completion\": EOF', 'type': 'api_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     30\u001b[0m time1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 31\u001b[0m doc_md \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_to_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplatform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time1\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mpdf_to_markdown\u001b[0;34m(pdf_source, method, reflection_provider, reflection_model)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Self-reflection for markdown heading corrections\u001b[39;00m\n\u001b[1;32m      9\u001b[0m reflection_client \u001b[38;5;241m=\u001b[39m get_llm_client(reflection_provider)\n\u001b[0;32m---> 10\u001b[0m new_headings \u001b[38;5;241m=\u001b[39m \u001b[43mget_correct_headings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreflection_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreflection_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_headings)\n\u001b[1;32m     12\u001b[0m doc_md \u001b[38;5;241m=\u001b[39m replace_headings(raw_md, new_headings) \n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mget_correct_headings\u001b[0;34m(doc_md, llm_client, llm_model)\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mIn the following markdown text, all the headers are on the same level. The top level headers are sections. Some sections may have\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124msub-sections or even sub-sub-sections. Please set the header levels correctly according to the content structures. For simplicity in your \u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124moutput, you can only response with all headers. Please consider the following rules:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m3. Please don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt add anything (such as level) that is not in the original headers. \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m\n\u001b[0;32m---> 12\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is the current markdown text:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdoc_md\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease export correct markdow headings, each per line.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m new_headings \u001b[38;5;241m=\u001b[39m responses\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_headings\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag_env/lib/python3.10/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': 'POST predict: Post \"http://127.0.0.1:42311/completion\": EOF', 'type': 'api_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "papers_folder = \"papers\"\n",
    "output_folder = \"output\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "timing_results = []\n",
    "\n",
    "pdf_files = [f for f in os.listdir(papers_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "models = [\n",
    "    # {\"platform\": \"OpenAI\", \"model\": \"gpt-4o\"},\n",
    "    {\"platform\": \"DeepSeek\", \"model\": \"deepseek-chat\"},\n",
    "    {\"platform\": \"Ollama\", \"model\": \"llama33-16k:latest\"}\n",
    "]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(papers_folder, pdf_file)\n",
    "    \n",
    "    for model in models:\n",
    "        output_filename = f\"{os.path.splitext(pdf_file)[0]}_{model['platform']}.md\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "        # Skip processing if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {output_filename}, already exists.\")\n",
    "            continue\n",
    "        \n",
    "        time1 = time.time()\n",
    "        doc_md = pdf_to_markdown(pdf_path, 'docling', model['platform'], model['model'])\n",
    "        elapsed_time = time.time() - time1\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(doc_md)\n",
    "        \n",
    "        timing_results.append({\n",
    "            \"PDF File\": pdf_file,\n",
    "            \"Platform\": model['platform'],\n",
    "            \"Model\": model['model'],\n",
    "            \"Time (s)\": elapsed_time\n",
    "        })\n",
    "\n",
    "        print(f\"Processed {pdf_file} with {model['model']} on {model['platform']} in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "timing_df = pd.DataFrame(timing_results)\n",
    "\n",
    "timing_csv_path = os.path.join(output_folder, \"timing_results.csv\")\n",
    "timing_df.to_csv(timing_csv_path, index=False)\n",
    "\n",
    "timing_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
